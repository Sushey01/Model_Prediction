# -*- coding: utf-8 -*-
"""Stroke_Prediction_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LZj1B_wUjlYYUpqckC4HpsYz3JHW8DYG
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
fedesoriano_stroke_prediction_dataset_path = kagglehub.dataset_download('fedesoriano/stroke-prediction-dataset')

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier
from sklearn.linear_model import LogisticRegression, Perceptron
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import kagglehub as kh
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import cross_val_score
import xgboost as xgb

"""# EDA

"""

# Load the dataset
df = pd.read_csv('healthcare-dataset-stroke-data.csv')

print(df.head)

print(df.shape)
print(df.info)
print(df.describe)

# Check for missing values
print(df.isnull().sum())

# Fill missing 'bmi' values with the mean
df['bmi'] = df['bmi'].fillna(df['bmi'].mean())

# Check again to ensure there are no missing values
print(df.isnull().sum())

# Target Variable Distribution
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(6,4))
sns.countplot(x=df['stroke'], palette="pastel")
plt.title("Stroke Occurrence Count")
plt.xlabel("Stroke (0: No, 1: Yes)")
plt.ylabel("Count")
plt.show()

# Age Distribution by Stroke Status
plt.figure(figsize=(12, 6))
sns.boxplot(x='stroke', y='age', data=df)
plt.title('Age Distribution by Stroke Status')
plt.xlabel('Stroke (1 = Yes, 0 = No)')
plt.ylabel('Age')
plt.xticks([0, 1], ['No', 'Yes'])
plt.show()

# Stroke Count by Gender
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x='gender', hue='stroke')
plt.title('Stroke Count by Gender')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.legend(title='Stroke', loc='upper right', labels=['No', 'Yes'])
plt.show()

# Average Glucose Level by Stroke Status
plt.figure(figsize=(12, 6))
sns.barplot(x='stroke', y='avg_glucose_level', data=df, estimator=np.mean)
plt.title('Average Glucose Level by Stroke Status')
plt.xlabel('Stroke (1 = Yes, 0 = No)')
plt.ylabel('Average Glucose Level')
plt.xticks([0, 1], ['No', 'Yes'])
plt.show()

# Visualizing Outliers
plt.figure(figsize=(12, 6))
sns.boxplot(data=df[['age', 'bmi', 'avg_glucose_level']])  # Only numerical features
plt.title('Boxplot for Outlier Detection')
plt.show()

# Capping Outliers (Winsorization)
for col in ['bmi', 'avg_glucose_level']:
    q1 = df[col].quantile(0.25)
    q3 = df[col].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
    df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])

# Stroke Occurrence by Work Type
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x='work_type', hue='stroke', palette='Set2')
plt.title('Stroke Occurrence by Work Type')
plt.xlabel('Work Type')
plt.ylabel('Count')
plt.legend(title='Stroke', labels=['No Stroke', 'Stroke'])
plt.xticks(rotation=45)
plt.show()

# Stroke vs Smoking Status
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x='smoking_status', hue='stroke', palette='Set1')
plt.title('Stroke Occurrence by Smoking Status')
plt.xlabel('Smoking Status')
plt.ylabel('Count')
plt.legend(title='Stroke', labels=['No Stroke', 'Stroke'])
plt.xticks(rotation=45)
plt.show()

# Correlation Heatmap
# Select only numeric columns
numeric_df = df.select_dtypes(include=['float64', 'int64'])

# Calculate the correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(numeric_df.corr(), annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""# FEATURE ENGINEERING"""

# One-Hot Encoding: This creates binary columns for each category.
df = pd.get_dummies(df, columns=['smoking_status'], drop_first=True)

# Creating Interaction Features
df['age_bmi_interaction'] = df['age'] * df['bmi']

# Binning Continuous Variables
df['age_group'] = pd.cut(df['age'], bins=[0, 30, 50, 70, 100], labels=['Young', 'Adult', 'Senior', 'Elderly'])

# Feature Scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df[['avg_glucose_level', 'bmi', 'age']] = scaler.fit_transform(df[['avg_glucose_level', 'bmi', 'age']])

# Removing Irrelevant or Redundant Features
df.drop(['id'], axis=1, inplace=True)

# Creating Derived Features
df['high_glucose'] = (df['avg_glucose_level'] > 140).astype(int)

# Check if 'gender' column still exists
print(df.columns)  # This will help you confirm the available columns

# Apply One-Hot Encoding correctly
df = pd.get_dummies(df, columns=['gender'], drop_first=False)  # Keep all categories
print(df.head())

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df[['age', 'avg_glucose_level', 'bmi']] = scaler.fit_transform(df[['age', 'avg_glucose_level', 'bmi']])

# Final check
print(df.head())
print(df.info())

# Separate features and target variable
X = df.drop('stroke', axis=1)  # Features
y = df['stroke']  # Target variable

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Print shapes to verify the split
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)

"""# MODEL TRAINING

1. DATA PREPARATION
"""

print(df.columns)
print(X_train.dtypes)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Assuming 'df' is your original DataFrame
# Identify categorical features that need encoding
categorical_features = ['ever_married', 'work_type', 'Residence_type', 'age_group']

# One-hot encode categorical variables
df_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=True)

# Separate features and target variable
X = df_encoded.drop('stroke', axis=1)  # Features
y = df_encoded['stroke']  # Target variable

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model (Random Forest example)
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)  # Train the model

# Make predictions and evaluate
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f'Accuracy: {accuracy:.2f}')

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn.ensemble import BaggingClassifier, VotingClassifier

# Define models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'Support Vector Machine': SVC(),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'K-Neighbors': KNeighborsClassifier(),
    'Naive Bayes': GaussianNB(),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    'AdaBoost': AdaBoostClassifier(random_state=42),
    'Bagging': BaggingClassifier(random_state=42),
    'Perceptron': Perceptron(max_iter=1000, random_state=42)
}

# Train Each Model and Collect Results
results = {}

for model_name, model in models.items():
    model.fit(X_train, y_train)  # Train the model
    y_pred = model.predict(X_test)  # Make predictions

    accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy
    results[model_name] = accuracy  # Store results


# Define the RandomForest model
rf_model = RandomForestClassifier(random_state=42)

# Define the hyperparameter grid
# Define the hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 200],  # Number of trees in the forest
    'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree
    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2, 4],    # Minimum number of samples required to be at a leaf node
    'max_features': ['auto', 'sqrt'],  # Number of features to consider when looking for the best split
}

# Set up the GridSearchCV
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, 
                           cv=5, n_jobs=-1, verbose=2, scoring='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
print("Best Parameters:", grid_search.best_params_)
print("Best Cross-Validation Score:", grid_search.best_score_)

# Use the best model from grid search
best_rf_model = grid_search.best_estimator_


# Convert results to DataFrame for better visualization
results_df = pd.DataFrame(list(results.items()), columns=['Model', 'Accuracy'])
results_df = results_df.sort_values(by='Accuracy', ascending=False)

print(results_df)

# Model Evaluation
from sklearn.metrics import confusion_matrix, classification_report

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# Classification Report
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

# Importing necessary libraries
from sklearn.model_selection import cross_val_score

# Step 3: Perform cross-validation and store results
cv_results = {}

for model_name, model in models.items():
    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')  # 5-fold cross-validation
    cv_results[model_name] = scores.mean()  # Store the mean accuracy

# Step 4: Convert results to DataFrame for better visualization
cv_results_df = pd.DataFrame(list(cv_results.items()), columns=['Model', 'CV Accuracy'])
cv_results_df = cv_results_df.sort_values(by='CV Accuracy', ascending=False)

print(cv_results_df)

# Step 5: Visualize cross-validation results
plt.figure(figsize=(10, 6))
plt.barh(cv_results_df['Model'], cv_results_df['CV Accuracy'], color='skyblue')
plt.xlabel('Cross-Validation Accuracy')
plt.title('Model Comparison with Cross-Validation')
plt.show()

# Step 6: Identify the best model
best_model_name = cv_results_df.iloc[0]['Model']
best_model_accuracy = cv_results_df.iloc[0]['CV Accuracy']
print(f"The best model is: {best_model_name} with an accuracy of {best_model_accuracy:.2f}")

# Step 7: Train the best model on the full training set
best_model = models[best_model_name]
best_model.fit(X_train, y_train)

# Step 8: Make predictions on the test set
y_pred = best_model.predict(X_test)

# Step 9: Evaluate the best model
# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# Classification Report
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

import pickle

# Assuming 'best_model' is your trained model
with open("best_model.pkl", "wb") as file:
    pickle.dump(best_model, file)

# Save the scaler object
# Save the scaler object
with open("scaler.pkl", "wb") as file:
    pickle.dump(scaler, file)
    
